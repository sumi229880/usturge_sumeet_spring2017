{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "# Use Gutenberg and Web_text data. Find out what are the top 5 words that            Shakespeare used but we do not use in currently.\n",
    "# Take top 50 words from Shakespeare (all 3 books) and top 50 from Web_text      (all the records).\n",
    "# Remove punctuation and stop words.\n",
    "# Remove the words we still use today, and get the unused list. Show the top 5      elements. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69340\n",
      "37032\n"
     ]
    }
   ],
   "source": [
    "emma1 = gutenberg.words('shakespeare-caesar.txt')\n",
    "emma2 = gutenberg.words('shakespeare-hamlet.txt')\n",
    "emma3 = gutenberg.words('shakespeare-macbeth.txt')\n",
    "Lis_Shakespeare = [emma1,emma2,emma3]\n",
    "\n",
    "newList_Shak = [] #list having all the words from all 3 books\n",
    "newList_Shak_wstop = [] #list without stopping words\n",
    "def shake_repo(inp_param):\n",
    "    for words in inp_param:\n",
    "        if words.isalpha():\n",
    "            newList_Shak.append(words.lower())\n",
    "\n",
    "for x in Lis_Shakespeare:\n",
    "    shake_repo(x)\n",
    "\n",
    "#removing the stopping words    \n",
    "for y in newList_Shak:\n",
    "    if y in stop_words:\n",
    "        continue\n",
    "    else:\n",
    "        newList_Shak_wstop.append(y)\n",
    "\n",
    "print(len(newList_Shak))\n",
    "print (len(newList_Shak_wstop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['firefox.txt',\n",
       " 'grail.txt',\n",
       " 'overheard.txt',\n",
       " 'pirates.txt',\n",
       " 'singles.txt',\n",
       " 'wine.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import webtext\n",
    "webtext.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306055\n",
      "175812\n"
     ]
    }
   ],
   "source": [
    "emma4 = webtext.words('firefox.txt')\n",
    "emma5 = webtext.words('grail.txt')\n",
    "emma6 = webtext.words('overheard.txt')\n",
    "emma7 = webtext.words('pirates.txt')\n",
    "emma8 = webtext.words('singles.txt')\n",
    "emma9 = webtext.words('wine.txt')\n",
    "Lis_webText = [emma4,emma5,emma6,emma7,emma8,emma9]\n",
    "\n",
    "newList_web = [] #list with all the words from different webtext\n",
    "newList_web_wstop = [] #list without stopping words\n",
    "def web_repo(inp_param):\n",
    "    for words in inp_param:\n",
    "        if words.isalpha():\n",
    "            newList_web.append(words.lower())\n",
    "\n",
    "for x in Lis_webText:\n",
    "    web_repo(x)\n",
    "\n",
    "for y in newList_web:\n",
    "    if y in stop_words:\n",
    "        continue\n",
    "    else:\n",
    "        newList_web_wstop.append(y)\n",
    "\n",
    "print(len(newList_web))\n",
    "print (len(newList_web_wstop))\n",
    "# print(newList_web[:10])\n",
    "# print(newList_Shak[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unused_words = []  #words which are not in webtext but there in shakespere\n",
    "for u in newList_Shak:\n",
    "    if u not in newList_web:\n",
    "        unused_words.append(u)\n",
    "       \n",
    "#unused_words = [z for u in newList_Shak if u not in newList_web]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5088\n"
     ]
    }
   ],
   "source": [
    "unused = {} #dictonary having used words from shakespere era\n",
    "for word in unused_words:\n",
    "    if word in unused:\n",
    "        unused[word] += 1\n",
    "    else:\n",
    "        unused[word] = 1\n",
    "# print(unused)        \n",
    "# print(len(unused_words)) \n",
    "# print(len(set(unused_words)))\n",
    "print(len(unused))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('haue', 448)\n",
      "('brutus', 162)\n",
      "('vpon', 162)\n",
      "('bru', 153)\n",
      "('hath', 144)\n"
     ]
    }
   ],
   "source": [
    "top_words = [(k, unused[k]) for k in sorted(unused, key=unused.get, reverse=True)]\n",
    "for k, v in top_words[:5]:\n",
    "    print((k, v)) #top 5 words with highest frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 2222)\n",
      "('and', 2036)\n",
      "('to', 1515)\n",
      "('i', 1455)\n",
      "('of', 1302)\n",
      "('you', 1124)\n",
      "('a', 1019)\n",
      "('my', 914)\n",
      "('that', 904)\n",
      "('haue', 896)\n",
      "('in', 826)\n",
      "('it', 778)\n",
      "('is', 769)\n",
      "('not', 722)\n",
      "('d', 662)\n",
      "('his', 588)\n",
      "('with', 557)\n",
      "('this', 546)\n",
      "('for', 533)\n",
      "('me', 529)\n",
      "('your', 528)\n",
      "('but', 510)\n",
      "('he', 491)\n",
      "('be', 476)\n",
      "('what', 458)\n",
      "('him', 434)\n",
      "('as', 427)\n",
      "('so', 424)\n",
      "('will', 384)\n",
      "('our', 346)\n",
      "('ham', 337)\n",
      "('vpon', 324)\n",
      "('brutus', 324)\n",
      "('all', 321)\n",
      "('thou', 312)\n",
      "('we', 306)\n",
      "('bru', 306)\n",
      "('are', 305)\n",
      "('s', 303)\n",
      "('shall', 300)\n",
      "('no', 297)\n",
      "('lord', 293)\n",
      "('hath', 288)\n",
      "('do', 287)\n",
      "('selfe', 286)\n",
      "('then', 278)\n",
      "('on', 276)\n",
      "('macb', 274)\n",
      "('heere', 270)\n",
      "('by', 256)\n"
     ]
    }
   ],
   "source": [
    "unused_Shak = {} #dictonary having used words from shakespere; these have stopping words as well, if we dont want them iterate over newList_Shak_wstop\n",
    "for word in newList_Shak:  \n",
    "    if word in unused:\n",
    "        unused[word] += 1\n",
    "    else:\n",
    "        unused[word] = 1\n",
    "        \n",
    "top_words = [(k, unused[k]) for k in sorted(unused, key=unused.get, reverse=True)]\n",
    "for k, v in top_words[:50]:\n",
    "    print((k, v)) #top 50 words with highest frequency        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 10131)\n",
      "('i', 9380)\n",
      "('to', 7840)\n",
      "('a', 7218)\n",
      "('you', 6936)\n",
      "('and', 6552)\n",
      "('in', 5119)\n",
      "('of', 4488)\n",
      "('it', 4163)\n",
      "('is', 3793)\n",
      "('that', 3696)\n",
      "('t', 3695)\n",
      "('s', 3623)\n",
      "('on', 3557)\n",
      "('not', 3387)\n",
      "('girl', 2956)\n",
      "('guy', 2751)\n",
      "('with', 2500)\n",
      "('my', 2472)\n",
      "('for', 2441)\n",
      "('this', 2037)\n",
      "('when', 1991)\n",
      "('but', 1957)\n",
      "('me', 1860)\n",
      "('like', 1858)\n",
      "('no', 1836)\n",
      "('what', 1820)\n",
      "('so', 1805)\n",
      "('be', 1794)\n",
      "('he', 1710)\n",
      "('are', 1533)\n",
      "('your', 1512)\n",
      "('was', 1421)\n",
      "('have', 1410)\n",
      "('if', 1355)\n",
      "('all', 1327)\n",
      "('do', 1321)\n",
      "('can', 1284)\n",
      "('we', 1249)\n",
      "('don', 1246)\n",
      "('from', 1219)\n",
      "('man', 1214)\n",
      "('know', 1201)\n",
      "('m', 1189)\n",
      "('at', 1158)\n",
      "('as', 1140)\n",
      "('they', 1104)\n",
      "('will', 1081)\n",
      "('his', 1058)\n",
      "('just', 1033)\n"
     ]
    }
   ],
   "source": [
    "unused_web = {} #dictonary having used words from webtext, these have stopping words as well, if we dont want them iterate over newList_web_wstop\n",
    "for word in newList_web:\n",
    "    if word in unused:\n",
    "        unused[word] += 1\n",
    "    else:\n",
    "        unused[word] = 1\n",
    "        \n",
    "top_words = [(k, unused[k]) for k in sorted(unused, key=unused.get, reverse=True)]\n",
    "for k, v in top_words[:50]:\n",
    "    print((k, v)) #top 50 words with highest frequency        "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
